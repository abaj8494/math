% optimisation-summaries.tex
\documentclass[12pt]{article}

\usepackage{amsmath, amssymb, amsfonts}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{margin=2.5cm}

\title{MATH3161/MATH5165 – Optimisation\\\large Consolidated Topic Summaries (Term 1 2025)}
\author{School of Mathematics \& Statistics, UNSW Sydney}
\date{}

\begin{document}
\maketitle
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Notation}
Unless stated otherwise, \(x\in\mathbb R^n\), \(f:\mathbb R^n\to\mathbb R\) is
continuously differentiable, \(\nabla f\) and \(\nabla^2 f\) denote the gradient
and Hessian respectively, and \(c_i\) are the constraint functions of a
non-linear programme
\[
\min_{x\in\mathbb R^n} f(x)\quad
\text{s.t. } c_i(x)=0\;(i\in E),\;
c_i(x)\le0\;(i\in I).
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 1 – Model Formulation}

\subsection{Standard form}
\[
\min_{x\in\mathbb R^n} f(x)\quad\text{s.t.}\quad
c_i(x)=0\;(i=1,\dots,m_E),\;
c_i(x)\le0\;(i=m_E\!+\!1,\dots,m).
\]

\paragraph{Typical conversions}
\begin{itemize}
  \item {\bf Maximisation.}\; \(\max f(x)= -\min\{-f(x)\}\).
  \item {\bf Right-hand sides.}\; \(c_i(x)=b_i\;\Longleftrightarrow\;c_i(x)-b_i=0\).
  \item {\bf ``\(\ge\)'' constraints.}\; \(c_i(x)\ge0\;\Longleftrightarrow\;-c_i(x)\le0\).
  \item {\bf Strict inequalities.}\; \(c_i(x)<0\;\Longleftrightarrow\;c_i(x)+\varepsilon\le0\)
        for some \(\varepsilon>0\).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 2 – Mathematical Background}

\subsection{Gradients and Hessians}
For \(f\in C^2(\mathbb R^n)\)
\[
\nabla f(x)=\begin{bmatrix}\partial f/\partial x_1\\[-2pt]\vdots\\[-2pt]\partial f/\partial x_n\end{bmatrix},
\qquad
\nabla^2 f(x)=
\Bigl[\partial^2 f/\partial x_i\partial x_j\Bigr]_{i,j=1}^n.
\]

\subsection{Definiteness of real matrices}
A (not necessarily symmetric) \(A\in\mathbb R^{n\times n}\) is
\begin{center}
\begin{tabular}{ll}
positive definite  & \(\iff x^\top Ax>0\;\forall x\ne0\),\\
positive semi-def. & \(\iff x^\top Ax\ge0\;\forall x\),\\
negative definite  & \(\iff x^\top Ax<0\;\forall x\ne0\),\\
negative semi-def. & \(\iff x^\top Ax\le0\;\forall x\),\\
indefinite         & \(\iff\exists x,z: x^\top Ax<0,\;z^\top Az>0\).
\end{tabular}
\end{center}

For a {\it symmetric} matrix the signs of the eigenvalues
\(\lambda_1,\dots,\lambda_n\) fully determine definiteness; e.g.\
\(A\succ0\iff\lambda_i>0\;\forall i\).
A convenient test for \(A\succ0\) is that all leading principal minors are
positive (Sylvester’s criterion).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 3 – Convexity of Sets and Functions}

\subsection{Sets}
\(\Omega\subseteq\mathbb R^n\) is {\bf convex} if
\(\theta x+(1-\theta)y\in\Omega\) for every \(x,y\in\Omega\) and
\(\theta\in[0,1]\).

\subsection{Functions}
A function \(f:\Omega\to\mathbb R\) (with \(\Omega\) convex) is
\begin{itemize}
  \item {\bf convex} if
        \(f(\theta x+(1-\theta)y)\le\theta f(x)+(1-\theta)f(y)\);
  \item {\bf strictly convex} if strict inequality holds whenever \(x\ne y\);
  \item {\bf concave} if \(-f\) is convex.
\end{itemize}

Useful characterisations:
\begin{align*}
f \text{ convex } &\iff
(\forall x,y\in\Omega)\;
f(y)\ge f(x)+\nabla f(x)^\top(y-x); \\
f \text{ convex on open }\Omega
 &\iff \nabla^2f(x)\succeq0\;\forall x\in\Omega;\\
\text{epigraph } \mathrm{epi}\,f
 &=\{(x,r): x\in\Omega,\,f(x)\le r\}\text{ is convex.}
\end{align*}

\bigskip

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 4 – Unconstrained Optimisation}

\subsection{First- and second-order tests}
For \(f\in C^1\):
\[
x^\ast\text{ local min}\;\Longrightarrow\; \nabla f(x^\ast)=0.
\]
For \(f\in C^2\):
\[
x^\ast\text{ local min}\;\Longrightarrow\;
\nabla f(x^\ast)=0,\;
\nabla^2 f(x^\ast)\succeq0.
\]
Moreover, if \(\nabla f(x^\ast)=0\) and \(\nabla^2 f(x^\ast)\succ0\) then
\(x^\ast\) is a {\it strict} local minimiser; \(\prec0\) gives a maximiser; an
indefinite Hessian implies a saddle.

For {\bf convex} (resp.\ concave) \(f\), {\em any} stationary point is a global
minimum (resp.\ maximum).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 5 – Equality-Constrained Optimisation}

\subsection{Problem}
\[
\min f(x)\quad\text{s.t. } c_i(x)=0\;(i=1,\dots,m).
\]

\paragraph{Lagrangian}
\(L(x,\lambda)=f(x)+\sum_{i=1}^{m}\lambda_i c_i(x)\).

\paragraph{Regularity}
A feasible \(x\) is {\bf regular} if \(\{\nabla c_i(x)\}_{i=1}^m\) are linearly
independent.

\paragraph{First-order (KKT) conditions}
If \(x^\ast\) is a local minimiser and regular, then
\[
\nabla_x L(x^\ast,\lambda^\ast)=0,\qquad c_i(x^\ast)=0.
\]
Any point satisfying these with some multipliers is a
{\bf constrained stationary point}.

\paragraph{Second-order sufficiency}
Let \(Z^\ast\) whose columns form a basis for \(\ker A^\top\) with
\(A=[\nabla c_1(x^\ast)\ \dots\ \nabla c_m(x^\ast)]\).  Define
\(W^\ast=\nabla^2 f(x^\ast)+\sum_{i=1}^m \lambda_i^\ast\nabla^2c_i(x^\ast)\).
If \((Z^\ast)^\top W^\ast Z^\ast\succ0\) then \(x^\ast\) is a strict local
minimum.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 6 – Problems with Inequality Constraints}

Given (NLP)
\[
\min f(x)\quad
\text{s.t.}\;
c_i(x)=0\ (i\in E),\;
c_i(x)\le0\ (i\in I),
\]
let the {\bf active\,set}
\(A(x)=\{i\in E\cup I : c_i(x)=0\}\).

\paragraph{KKT\,conditions}
At a regular local minimiser \(x^\ast\) there exist multipliers
\(\lambda^\ast\) such that
\[
\nabla f(x^\ast)+\sum_{i\in A(x^\ast)}\lambda_i^\ast\,\nabla c_i(x^\ast)=0,
\quad
\lambda_i^\ast\ge0\;(i\in I\cap A(x^\ast)).
\]

\paragraph{Second-order test}
With \(Z^\ast\), \(W^\ast\) defined as before and
\(t^\ast=|A(x^\ast)|<n\):
if \(\lambda_i^\ast>0\;\forall i\in I\cap A(x^\ast)\) and
\((Z^\ast)^\top W^\ast Z^\ast\succ0\) then \(x^\ast\) is a strict local
minimum.

\subsection{Convex programmes}
If \(f\) is convex, \(c_i\) affine (\(i\in E\)), and \(c_i\) convex
(\(i\in I\)), then {\em any} point satisfying the KKT conditions with
\(\lambda_i\ge0\;(i\in I)\) is a {\bf global} minimiser.

\subsection{Wolfe dual}
For \(m=|E\cup I|\)
\[
\max_{y,\lambda}\;
f(y)+\sum_{i=1}^m\lambda_i c_i(y)\;
\text{s.t. } \nabla f(y)+\sum_{i=1}^m\lambda_i\nabla c_i(y)=0,\;
\lambda_i\ge0\ (i\in I).
\]
Strong duality holds in the convex case.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 7 – Numerical Methods (Unconstrained)}

\subsection{General line-search framework}
Given descent direction \(s^{(k)}\) at \(x^{(k)}\), choose
\(\alpha^{(k)}>0\) (exact or inexact) and set
\(x^{(k+1)}=x^{(k)}+\alpha^{(k)}s^{(k)}\).

\paragraph{Convergence rates}
If \(x^{(k)}\to x^\ast\) and
\(\|x^{(k+1)}-x^\ast\|/\|x^{(k)}-x^\ast\|^{\alpha}\to\beta\)
then the method is {\em \(\alpha\)-th-order}:
\(\alpha=1\) linear, \(\alpha=1,\beta=0\) super-linear,
\(\alpha=2\) quadratic.

\subsection{Steepest Descent}
\[
s^{(k)}=-\nabla f(x^{(k)}).
\]
Globally convergent, linear rate in the quadratic case; no quadratic
termination.

\subsection{Newton’s method}
\[
\nabla^{2}f(x^{(k)})\,\delta^{(k)}=-\nabla f(x^{(k)}),\quad
s^{(k)}=\delta^{(k)}.
\]
Quadratic convergence near a non-singular minimiser; single-step
termination for strictly convex quadratics; may fail globally if
\(\nabla^{2}f\) is singular or indefinite.

\subsection{Conjugate Gradient (non-linear CG)}
\[
s^{(k)}=-g^{(k)}+\beta^{(k)}s^{(k-1)},\quad
g^{(k)}=\nabla f(x^{(k)}).
\]
Descent directions, quadratic termination (exact line search), especially
attractive for large-scale problems because only vector operations are
required.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 8 – Penalty Function Methods}

For (P) with mixed constraints define
\[
P(x)=\sum_{i\in E} c_i(x)^2 +
       \sum_{i\in I} \bigl[c_i(x)\bigr]_+^2,\qquad
[x]_+=\max\{x,0\}.
\]
The penalty subproblem
\[
\min_{x\in\mathbb R^n} f(x)+\mu P(x)\quad(:=P_\mu)
\]
is unconstrained.  Under mild boundedness assumptions, every sequence of
minimisers \(x_\mu\) with \(\mu\to\infty\) has accumulation points that
solve the original constrained problem, and \(\mu P(x_\mu)\to0\).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Topic 9 – Optimal Control (Pontryagin Maximum Principle)}

For an autonomous system with fixed end-points
\[
\min_{u(\cdot)}\int_{t_0}^{t_1} f_0\bigl(x(t),u(t)\bigr)\,dt,\quad
\dot x=f\bigl(x(t),u(t)\bigr),\;
x(t_0)=x_0,\;x(t_1)=x_1,
\]
define the Hamiltonian
\(H(x,\hat z,u)=\hat z^\top\!\bigl(f_0(x,u),f(x,u)\bigr)\).
There exists a non-trivial adjoint \(\hat z(t)\) with
\(\dot{\hat z}=-\partial H/\partial x\) such that the optimal control
\(u^\ast(t)\) maximises \(H\bigl(x^\ast(t),\hat z(t),u\bigr)\) for all
\(u\in U\).  For fixed end-time the Hamiltonian is constant along the
optimal trajectory; it vanishes when the terminal time is free.  If only
some components of \(x(t_1)\) are fixed, a transversality condition
relates adjoint values to the gradients of the terminal constraints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

