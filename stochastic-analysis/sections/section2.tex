\section{Stochastic Processes: Preliminaries}

\subsection{Stochastic Processes}

\begin{definition}[stochastic process]
For a given probability space $(\Omega, \A, \P)$, a (real-valued) \textbf{stochastic process} $(X_t)_{t \in I}$ is a collection of $\A$-measurable random variables $X_t$ where $t \in I$ (known as the index set). It can also be written as $(X_{t,\omega})_{t \in I, \omega \in \Omega}$ to reflect that it is actually a function of two variables mapping from $I \times \Omega \to \R$.
\end{definition}

\begin{definition}[filtration, $\F$-adapted process, natural filtration]
\leavevmode
\begin{itemize}
    \item A \textbf{filtration} $\F = (\F_t)_{t \in [0,T]}$ is an increasing family of $\sigma$-algebras, that is, $\F_u \subseteq \F_t$ for any $0 \leq u \leq t \leq T$.
    \item A stochastic process $X = (X_t)_{t \in [0,T]}$ defined on $(\Omega, \F, \P)$ is \textbf{$\F$-adapted} if for any $t \in [0,T]$, the random variable $X_t$ is $\F_t$-measurable, i.e.\ for any $x \in \mathbb{Q}$, the event $\{X_t \leq x\} \in \F_t$ (Corollary~1.3.1).
    \item The \textbf{natural filtration} of $X$ (or the filtration generated by $X$) is defined as $\F^X = (\F^X_t)_{t \in [0,T]}$ where $\F^X_t = \sigma(X_u, u \leq t)$. Any stochastic process $X$ is, by definition, adapted to its natural filtration.
\end{itemize}
\end{definition}

\begin{remark}
We assume that $\A = \F_T$ and hereafter we shall write $(\Omega, \F, \P)$ instead of $(\Omega, \A, \F, \P)$.
\end{remark}

\begin{definition}[``equality'' of stochastic processes]
Two processes $X$ and $Y$ defined on a common probability space are said to be
\begin{itemize}
    \item (stronger) \textbf{indistinguishable} if the event $\{X_t = Y_t, \text{ for all } t \in [0,T]\}$ has probability 1, i.e.\ $\P\!\left(\bigcap_t \{X_t = Y_t\}\right) = 1$.
    \item (weaker) \textbf{modifications} of each other if for all $t \geq 0$, $\P(X_t = Y_t) = 1$.
\end{itemize}
\end{definition}

\begin{remark}
\leavevmode
\begin{enumerate}
    \item $X$ and $Y$ are indistinguishable $\Rightarrow$ $X$ and $Y$ are modifications of each other, but not the reverse.
    \item In discrete time these two concepts coincide.
\end{enumerate}
\end{remark}

\begin{proof}
We only need to show the converse. Since $X$ and $Y$ are modifications of each other, $\P(X_t = Y_t) = 1$ for all $t$ in some countable set $I$. Since countable unions of null sets are again null sets\footnote{Follows from countable additivity of probability measure (Definition~1.2.2).}, countable intersections of sets with full measure, have again full measure. Hence, $\P\!\left(\bigcap_{t \in I} \{X_t = Y_t\}\right) = 1$.
\end{proof}

\begin{enumerate}
    \setcounter{enumi}{2}
    \item If $X$ and $Y$ two c\`adl\`ag processes are modification of each other, then $X$ and $Y$ are indistinguishable.
\end{enumerate}

\begin{examples}[Counterexample of the converse]
We give an example where $X$ and $Y$ are modifications of each other, but not distinguishable. Consider the space $([0,1], \B([0,1]), \P)$, where $\P$ is the Lebesgue measure, i.e.\ $\P([a,b]) = b - a$. Let $(Y_t)_{t \in [0,1]}$ denote a constant process given by $Y_t = 0$ and $(X_t)_{t \in [0,1]}$ given by
\[
X_t(\omega) = \begin{cases} 1 & \text{if } t = \omega, \\ 0 & \text{if } t \neq \omega. \end{cases}
\]

For a fixed $\omega$, the trajectories $X_t(\omega)$ and $Y_t(\omega)$ differs only at the point $t = \omega$. To see that $X$ and $Y$ are modifications of each other, for every $t \in [0,1]$, we have
\[
\{\omega : X_t(\omega) = Y_t(\omega)\} = \{\omega : \omega \neq t\} = \Omega \setminus \{\omega : \omega = t\}
\]
This shows that $\P(X_t = Y_t) = 1 - \P(\{t\}) = 1$. Here the process $X$ is not a right continuous process (c\`adl\`ag), therefore one cannot conclude that $X$ and $Y$ are indistinguishable. In fact, we see that
\[
\{\omega : X_t(\omega) = Y_t(\omega), \text{ for all } t \in [0,1]\} = \bigcap_{t \in [0,1]} \{\omega : X_t(\omega) = Y_t(\omega)\} = \emptyset
\]
since the complement is given by
\[
\bigcup_{t \in [0,1]} \{\omega : X_t(\omega) \neq Y_t(\omega)\} = \bigcup_{t \in [0,1]} \{\omega : \omega = t\} = \Omega
\]
which means $(X_t)_{t \in [0,1]}$ and $(Y_t)_{t \in [0,1]}$ cannot be indistinguishable.
\end{examples}

\subsection{Martingales}

\begin{definition}[martingales, submartingales, supermartingales]
Consider a real-valued, $\F$-adapted process $M = (M_t)_{t \in [0,T]}$, defined on a filtered probability space $(\Omega, \F, \P)$. If
\begin{enumerate}[(i)]
    \item $M$ is integrable, that is, $\E|M_t| < \infty$ for $t \in [0,T]$, and
    \item (martingale property) for any $0 \leq s \leq t \leq T$,
    \begin{itemize}
        \item $\E(M_t|\F_s) = M_s$, then $M$ is an $\F$-\textbf{martingale}. The expectation is a constant: $\E(M_t) = \E(M_0)$, for all $t \in [0,T]$.
        \item $\E(M_t|\F_s) \geq M_s$, then $M$ is an $\F$-\textbf{submartingale}. The expectation is increasing: $\E(M_t) \geq \E(M_0)$ for any $t \in [0,T]$.
        \item $\E(M_t|\F_s) \leq M_s$, then $M$ is an $\F$-\textbf{supermartingale}. The expectation is decreasing: $\E(M_t) \leq \E(M_0)$ for any $t \in [0,T]$.
    \end{itemize}
\end{enumerate}
\end{definition}

\begin{remark}
(equivalent conditions for discrete time martingales) For a discrete time process, it suffices to show
\begin{itemize}
    \item $\E(M_{n+1}|\F_n) = M_n$ for all $n = 0, 1, \ldots, N-1$, or
    \item $\E(M_N|\F_n) = M_n$ for all $n = 0, 1, \ldots, N$.
\end{itemize}
Other ways to prove a martingale in continuous case include:
\begin{itemize}
    \item Lemma~2.2.1: A sub- or supermartingale with constant expectation.
    \item $\E(M_T|\F_t) = M_t$ for all $t \in [0,T]$.
    \item Corollary~3.3.2: $M^2 - \langle M \rangle$ is an $\F$-martingale if $M$ is a continuous, square integrable $\F$-martingale.
    \item Theorem~4.1.1: $I(\gamma)$ for $\gamma \in L^2_\P(W)$ is an $\F$-martingale.
    \item Theorem~4.2.1: $I(\gamma)$ for $\gamma \in L_\P(W)$ is an $\F$-local martingale.
    \item It\^o's lemma (Theorem~4.3.1) proves a continuous semimartingale when $g$ is sufficiently smooth.
\end{itemize}
\end{remark}

\begin{lemma}
Assume that $M$ is either an $\F$-submartingale or an $\F$-supermartingale. Then $M$ is an $\F$-martingale if and only if the expected value of $M$ is constant, i.e.\ $\E[M_0] = \E[M_t]$ for all $t \in [0,T]$.
\end{lemma}

\begin{proof}
The only if part is straightforward. We prove only the converse.

Suppose $M$ is a submartingale with constant expectation. Let $0 \leq u \leq t \leq T$. Then $\E[M_t|\F_u] - M_u \geq 0$ and
\[
\E[\E[M_t|\F_u] - M_u] = \E[M_t] - \E[M_u] = 0
\]
by assumption. A non-negative random variable $X$ with $\E(X) = 0$ almost surely, as the Markov inequality implies $\P(X \geq 2^{-n}) \leq 2^n \E(X) = 0$.
This implies that $\E[M_t|\F_u] = M_u$ almost surely, i.e.\ $M$ is an $\F$-martingale.
\end{proof}

\begin{lemma}[convex transform of a martingale]
Let $M$ be an $\F$-martingale and let $h : \R \to \R$ be a convex function. If the process $X = h(M)$ is integrable, then it is an $\F$-submartingale.
\end{lemma}

\begin{examples}[Conditional Jensen's inequality (Theorem~1.5.2)]
Conditional Jensen's inequality can come handy when dealing with martingales. For example, if $S$ is a martingale, then the process $(S - K)^+$ is a submartingale, as $f : x \mapsto (x - K)^+$ is convex:
\[
\E[f(S_t)|\F_s] \geq f(\E[S_t|\F_s]) = f(S_s),
\]
for $0 \leq s \leq t \leq T$.
Similarly we can show that $(S^2)$ is also a submartingale.
\end{examples}

\begin{lemma}[conditional expectation process is a martingale]
Let $X$ be an $\F_T$-measurable and integrable random variable. Then the process defined by $M_t = \E(X|\F_t)$ for $t \in [0,T]$ is an $\F$-martingale.
\end{lemma}

\begin{proof}
This is an immediate result of the tower property.
\end{proof}

\subsection{Local Martingales}

\begin{definition}[local martingales]
A process $M$ is an $\F$-\textbf{local martingale} if there exists an increasing sequence $(\tau_n)_{n \in \mathbb{N}}$ of stopping times such that $\tau_n \to T$ a.s.\ ($T$ can be $\infty$), and for every $n$ the stopped process $M^{\tau_n}$ is a uniformly integrable martingale. Any sequence $(\tau_n)_{n \in \mathbb{N}}$ with these properties is called the \textbf{reducing sequence} for a local martingale $M$.
\end{definition}

\subsection{Doob Decomposition Theorem}

\subsubsection{Discrete Time: Doob Decomposition}

\begin{theorem}[Doob decomposition theorem]
A discrete time submartingale $X = (X_n)_{n=0,1,\ldots,N}$ can be decomposed into
\[
X_n = M_n + A_n
\]
where $M$ is a martingale and $A$ is a predictable, increasing process with $A_0 = 0$. Similarly, a supermartingale can be decomposed into a martingale and a decreasing process. This decomposition is almost surely unique.
\end{theorem}

\begin{proof}
\textbf{Existence.} We prove the existence of Doob decomposition by construction. Define the processes $A$ and $M$ by setting, for every $n \in I = \{0, 1, \ldots, N\}$,
\[
A_n = \sum_{k=1}^{n} \left(\E[X_k|\F_{k-1}] - X_{k-1}\right)
\]
and
\[
M_n = X_0 + \sum_{k=1}^{n} \left(X_k - \E[X_k|\F_{k-1}]\right).
\]

It is easy to check that $X_n = M_n + A_n$ for all $n \in I$, by writing $X_n - X_0$ as a telescoping sum.

It is clear that $A$ is increasing since $\E[X_k|\F_{k-1}] - X_{k-1} \geq 0$. To check that $M$ is a martingale, we note that
\begin{align*}
\E[M_n|\F_{n-1}] &= \E[\hblue{M_{n-1} + X_n - \E[X_n|\F_{n-1}]} \mid \F_{n-1}] \\
&= M_{n-1} + \E[X_n|\F_{n-1}] - \E[X_n|\F_{n-1}] \\
&= M_{n-1}
\end{align*}
for all $n = 1, \ldots, N$.

\textbf{Uniqueness.} Let $X = M' + A'$ be an additional decomposition. Then the process $Y := M - M' = A' - A$ is a martingale, implying that
\[
\E[Y_n|\F_{n-1}] = Y_{n-1},
\]
and also predictable (as $A$ is predictable), implying that
\[
\E[Y_n|\F_{n-1}] = Y_n,
\]
for any $n = 1, \ldots, N$. Since $Y_0 = A'_0 - A_0 = 0$ by the convention about the starting point of the predictable processes, this implies iteratively that $Y_n = 0$ almost surely for all $n = 0, 1, \ldots, N$.
\end{proof}

\subsubsection{Continuous Time: Doob--Meyer Decomposition}

\begin{theorem}[Doob--Meyer decomposition theorem]
Let $X$ be a non-negative continuous submartingale\footnote{A more general version of the theorem applies to any c\`adl\`ag supermartingale of class D.}. Then there exists a continuous increasing process $A$ with $A_0 = 0$ such that the process $M = X - A$ is a continuous martingale. The processes $M$ and $A$ in the Doob--Meyer decomposition
\[
X = M + A
\]
are unique up to indistinguishability of stochastic processes (Definition~2.1.3).
\end{theorem}

\subsection{Stopping Times}

\begin{definition}[stopping time]
An $\F$-\textbf{stopping time} $\tau$ is a random variable taking values in $[0, \infty]$ such that
\[
\{\tau \leq t\} \in \F_t
\]
for all $t \geq 0$.
\end{definition}

\begin{lemma}[stopped processes in discrete time, p.26]
\leavevmode
\begin{itemize}
    \item Let $X = (X_n)_{n=0,\ldots,N}$ be an $\F$-adapted process and $\tau$ be an $\F$-stopping time. Then the stopped process
    \[
    X^\tau := (X_{n \wedge \tau}) = X_n \indic_{n < \tau} + X_\tau \indic_{n \geq \tau}
    \]
    is also $\F$-adapted.
    \item Let $M = (M_n)_{n=0,\ldots,N}$ be an $\F$-martingale process and $\tau$ be an $\F$-stopping time. Then the stopped process
    \[
    M^\tau := (M_{n \wedge \tau}) = M_n \indic_{n < \tau} + M_\tau \indic_{n \geq \tau}
    \]
    is also an $\F$-martingale.
\end{itemize}
\end{lemma}

\begin{definition}
Given a stopping time $\tau$, the $\sigma$-algebra $\F_\tau$ defined by
\[
\F_\tau = \{A \in \F_T : \text{for all } t \in [0,T], \; A \cap \{\tau \leq t\} \in \F_t\}
\]
is called the \textbf{$\sigma$-algebra of events prior to $\tau$}.
\end{definition}

\begin{theorem}[Doob's optional sampling theorem]
Let $\sigma \leq \tau$ be two stopping times taking values in $\{0, 1, \ldots, N\}$. For any martingale, submartingale or supermartingale $X$, we have
\begin{enumerate}[(i)]
    \item The random variables $X_\sigma$ and $X_\tau$ are integrable.
    \item $\E[X_\tau|\F_\sigma] = X_\sigma$ (if $X$ is a martingale), $\E[X_\tau|\F_\sigma] \geq X_\sigma$ (submartingale) and $\E[X_\tau|\F_\sigma] \leq X_\sigma$ (supermartingale) holds respectively.
\end{enumerate}
\end{theorem}

\begin{lemma}[a discrete time result]
Let $M = (M_n)_{n=0,1,\ldots,N}$ be an $\F$-adapted and integrable stochastic process. Then $M$ is an $\F$-martingale if and only if $\E(M_\tau) = \E(M_0)$ for any stopping time $\tau$ with values in $\{0, 1, \ldots, N\}$.
\end{lemma}

\begin{proof}
($\Rightarrow$) This is a direct result of Lemma~2.2.1.

($\Leftarrow$) By tower property, it suffices to check that $M_n = \E(M_N|\F_n)$ for every $n \in \{0, 1, \ldots, N\}$. By assumption, we have $\E(M_\tau) = \E(M_N)$ for any stopping time $\tau$ with values in $\{0, 1, \ldots, N\}$ (this is true since $\tau = N$ is also a stopping time). Let us fix $t$ and consider an event $A \in \F_t$. Define $\tau_A$ as
\[
\tau_A = \begin{cases} n, & \text{if } \omega \in A, \\ N, & \text{if } \omega \notin A, \end{cases}
\]
then $\tau_A$ is an $\F$-stopping time with values in $\{0, 1, \ldots, N\}$ and so $\E(M_{\tau_A}) = \E(M_N)$. This yields $\E(\indic_A M_n) = \E(\indic_A M_N)$. Since this equality holds for any event $A \in \F_t$, by definition we have $M_n = \E(M_N|\F_n)$, which in turn implies that $M$ is an $\F$-martingale.
\end{proof}
